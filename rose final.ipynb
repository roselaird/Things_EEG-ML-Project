{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Report Rose Laird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data_dir_root = os.path.join('ThingsEEG-Text')\n",
    "sbj = 'sub-10'\n",
    "image_model = 'pytorch/cornet_s'\n",
    "text_model = 'CLIPText'\n",
    "roi = '17channels'\n",
    "brain_dir = os.path.join(data_dir_root, 'brain_feature', roi, sbj)\n",
    "image_dir_seen = os.path.join(data_dir_root, 'visual_feature/ThingsTrain', image_model, sbj)\n",
    "image_dir_unseen = os.path.join(data_dir_root, 'visual_feature/ThingsTest', image_model, sbj)\n",
    "text_dir_seen = os.path.join(data_dir_root, 'textual_feature/ThingsTrain/text', text_model, sbj)\n",
    "text_dir_unseen = os.path.join(data_dir_root, 'textual_feature/ThingsTest/text', text_model, sbj)\n",
    "\n",
    "brain_seen = sio.loadmat(os.path.join(brain_dir, 'eeg_train_data_within.mat'))['data'].astype('double') * 2.0\n",
    "brain_seen = brain_seen[:,:,27:60] # 70ms-400ms\n",
    "brain_seen = np.reshape(brain_seen, (brain_seen.shape[0], -1))\n",
    "image_seen = sio.loadmat(os.path.join(image_dir_seen, 'feat_pca_train.mat'))['data'].astype('double')*50.0\n",
    "text_seen = sio.loadmat(os.path.join(text_dir_seen, 'text_feat_train.mat'))['data'].astype('double')*2.0\n",
    "label_seen = sio.loadmat(os.path.join(brain_dir, 'eeg_train_data_within.mat'))['class_idx'].T.astype('int')\n",
    "image_seen = image_seen[:,0:100]\n",
    "\n",
    "brain_unseen = sio.loadmat(os.path.join(brain_dir, 'eeg_test_data.mat'))['data'].astype('double')*2.0\n",
    "brain_unseen = brain_unseen[:, :, 27:60]\n",
    "brain_unseen = np.reshape(brain_unseen, (brain_unseen.shape[0], -1))\n",
    "image_unseen = sio.loadmat(os.path.join(image_dir_unseen, 'feat_pca_test.mat'))['data'].astype('double')*50.0\n",
    "text_unseen = sio.loadmat(os.path.join(text_dir_unseen, 'text_feat_test.mat'))['data'].astype('double')*2.0\n",
    "label_unseen = sio.loadmat(os.path.join(brain_dir, 'eeg_test_data.mat'))['class_idx'].T.astype('int')\n",
    "image_unseen = image_unseen[:, 0:100]\n",
    "\n",
    "brain_seen = torch.from_numpy(brain_seen)\n",
    "brain_unseen = torch.from_numpy(brain_unseen)\n",
    "image_seen = torch.from_numpy(image_seen)\n",
    "image_unseen = torch.from_numpy(image_unseen)\n",
    "text_seen = torch.from_numpy(text_seen)\n",
    "text_unseen = torch.from_numpy(text_unseen)\n",
    "label_seen = torch.from_numpy(label_seen)\n",
    "label_unseen = torch.from_numpy(label_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "index_seen = np.squeeze(np.where(label_seen < 21, True, False))\n",
    "index_unseen = np.squeeze(np.where(label_unseen < 21, True, False))\n",
    "\n",
    "brain_seen = brain_seen[index_seen, :]\n",
    "image_seen = image_seen[index_seen, :]\n",
    "text_seen = text_seen[index_seen, :]\n",
    "label_seen = label_seen[index_seen]\n",
    "brain_unseen = brain_unseen[index_unseen, :]\n",
    "image_unseen = image_unseen[index_unseen, :]\n",
    "text_unseen = text_unseen[index_unseen, :]\n",
    "label_unseen = label_unseen[index_unseen]\n",
    "\n",
    "num_classes = 20\n",
    "samples_per_class = 10\n",
    "\n",
    "new_train_brain = []\n",
    "new_train_image = []\n",
    "new_train_text = []\n",
    "new_train_label = []\n",
    "\n",
    "new_test_brain = []\n",
    "new_test_image = []\n",
    "new_test_text = []\n",
    "new_test_label = []\n",
    "count = 0\n",
    "\n",
    "\n",
    "for i in range(num_classes):\n",
    "    start_idx = i * samples_per_class#The starting index of the current class\n",
    "    end_idx = start_idx + samples_per_class#The end index of the current class\n",
    "    #Get the data of the current class\n",
    "    class_data_brain = brain_seen[start_idx:end_idx, :]\n",
    "    #Divided into training set and test set\n",
    "    new_train_brain.append(class_data_brain[:7])\n",
    "    new_test_brain.append(class_data_brain[7:])\n",
    "\n",
    "    class_data_image = image_seen[start_idx:end_idx, :]\n",
    "\n",
    "    new_train_image.append(class_data_image[:7])\n",
    "    new_test_image.append(class_data_image[7:])\n",
    "\n",
    "    class_data_text = text_seen[start_idx:end_idx, :]\n",
    "\n",
    "    new_train_text.append(class_data_text[:7])\n",
    "    new_test_text.append(class_data_text[7:])\n",
    "\n",
    "    class_data_label = label_seen[start_idx:end_idx, :]\n",
    "\n",
    "    new_train_label.append(class_data_label[:7])\n",
    "    new_test_label.append(class_data_label[7:])\n",
    "  \n",
    "\n",
    "train_brain = torch.vstack(new_train_brain)\n",
    "train_image = torch.vstack(new_train_image)\n",
    "train_text = torch.vstack(new_train_text)\n",
    "train_label = torch.vstack(new_train_label)\n",
    "test_brain = torch.vstack(new_test_brain)\n",
    "test_image = torch.vstack(new_test_image)\n",
    "test_text = torch.vstack(new_test_text)\n",
    "test_label = torch.vstack(new_test_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Making Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_brain_np = train_brain.numpy()\n",
    "train_image_np = train_image.numpy()\n",
    "train_text_np = train_text.numpy()\n",
    "train_label_np = train_label.numpy().ravel()\n",
    "\n",
    "test_brain_np = test_brain.numpy()\n",
    "test_image_np = test_image.numpy()\n",
    "test_text_np = test_text.numpy()\n",
    "test_label_np = test_label.numpy().ravel()\n",
    "\n",
    "train_features_multiple = np.hstack((train_brain_np, train_image_np, train_text_np))\n",
    "test_features_multiple = np.hstack((test_brain_np, test_image_np, test_text_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapes and Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Feature Modality Breakdown:\")\n",
    "print(f\"Brain Features: {train_brain.shape[1]}\")\n",
    "print(f\"Image Features: {train_image.shape[1]}\")\n",
    "print(f\"Text Features: {train_text.shape[1]}\")\n",
    "\n",
    "train_samples = len(train_features_multiple)\n",
    "test_samples = len(test_features_multiple)\n",
    "classes = train_label.unique()\n",
    "\n",
    "print(f\"Training Samples: {train_samples}\")\n",
    "print(f\"Testing Samples: {test_samples}\")\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "train_class_dist = pd.Series(train_label_np).value_counts()\n",
    "test_class_dist = pd.Series(test_label_np).value_counts()\n",
    "\n",
    "print(\"\\nTraining Class Distribution:\")\n",
    "print(train_class_dist)\n",
    "\n",
    "print(\"\\nTesting Class Distribution:\")\n",
    "print(test_class_dist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean, Standard Deviation and Range Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_tensor_statistics(tensor, tensor_name):\n",
    "    mean = tensor.mean(dim=0)\n",
    "    std = tensor.std(dim=0)\n",
    "    feature_range = tensor.max(dim=0).values - tensor.min(dim=0).values\n",
    "    \n",
    "    stats_df = pd.DataFrame({\n",
    "        \"Feature\": range(mean.shape[0]),\n",
    "        \"Mean\": mean.numpy(),\n",
    "        \"Standard Deviation\": std.numpy(),\n",
    "        \"Range\": feature_range.numpy()\n",
    "    })\n",
    "    return stats_df\n",
    "\n",
    "train_brain_stats = calculate_tensor_statistics(train_brain, \"train_brain\")\n",
    "train_text_stats = calculate_tensor_statistics(train_text, \"train_text\")\n",
    "trainimage_stats = calculate_tensor_statistics(train_image, \"train_image\")\n",
    "\n",
    "aggregate_stats = pd.DataFrame({\n",
    "    \"Dataset\": [\"train_brain\", \"train_text\", \"trainimage\"],\n",
    "    \"Mean of Means\": [\n",
    "        train_brain_stats[\"Mean\"].mean(),\n",
    "        train_text_stats[\"Mean\"].mean(),\n",
    "        trainimage_stats[\"Mean\"].mean()\n",
    "    ],\n",
    "    \"Mean of Standard Deviations\": [\n",
    "        train_brain_stats[\"Standard Deviation\"].mean(),\n",
    "        train_text_stats[\"Standard Deviation\"].mean(),\n",
    "        trainimage_stats[\"Standard Deviation\"].mean()\n",
    "    ],\n",
    "    \"Mean of Ranges\": [\n",
    "        train_brain_stats[\"Range\"].mean(),\n",
    "        train_text_stats[\"Range\"].mean(),\n",
    "        trainimage_stats[\"Range\"].mean()\n",
    "    ]\n",
    "})\n",
    "\n",
    "train_brain_stats.to_csv(\"train_brain_stats.csv\", index=False)\n",
    "train_text_stats.to_csv(\"train_text_stats.csv\", index=False)\n",
    "trainimage_stats.to_csv(\"trainimage_stats.csv\", index=False)\n",
    "aggregate_stats.to_csv(\"aggregate_stats.csv\", index=False)\n",
    "\n",
    "print(\"Train Brain Statistics:\\n\", train_brain_stats.head())\n",
    "print(\"Train Text Statistics:\\n\", train_text_stats.head())\n",
    "print(\"Train Image Statistics:\\n\", trainimage_stats.head())\n",
    "print(\"Aggregate Statistics:\\n\", aggregate_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_dist.plot(kind='bar', title='Training Class Distribution', color='skyblue')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "test_class_dist.plot(kind='bar', title='Testing Class Distribution', color='orange')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_brain_pd = pd.DataFrame(train_brain)\n",
    "train_image_pd = pd.DataFrame(train_image)\n",
    "train_text_pd = pd.DataFrame(train_text)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(train_brain_pd.corr(), cmap='coolwarm', annot=False)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Correlation Heatmap: Brain Features\")\n",
    "plt.show()\n",
    "\n",
    "sns.heatmap(train_image_pd.corr(), cmap='coolwarm', annot=False)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Correlation Heatmap: Image Features\")\n",
    "plt.show()\n",
    "\n",
    "sns.heatmap(train_text_pd.corr(), cmap='coolwarm', annot=False)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Correlation Heatmap: Text Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers -PERCHANCE NOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "outlier_mask = iso.fit_predict(train_features_multiple) == 1 \n",
    "\n",
    "X_train_filtered = train_features_multiple[outlier_mask]\n",
    "y_train_filtered = train_label_np[outlier_mask]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_unfiltered_scaled = scaler.fit_transform(train_features_multiple)\n",
    "X_filtered_scaled = scaler.transform(X_train_filtered)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_unfiltered_pca = pca.fit_transform(X_unfiltered_scaled) \n",
    "X_filtered_pca = pca.fit_transform(X_filtered_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "\n",
    "axes[0].scatter(\n",
    "    X_unfiltered_pca[:, 0],\n",
    "    X_unfiltered_pca[:, 1],\n",
    "    c=train_label_np, \n",
    "    cmap='viridis',\n",
    "    alpha=0.6\n",
    ")\n",
    "axes[0].set_title(\"Before Outlier Removal\")\n",
    "axes[0].set_xlabel(\"PCA Component 1\")\n",
    "axes[0].set_ylabel(\"PCA Component 2\")\n",
    "\n",
    "axes[1].scatter(\n",
    "    X_filtered_pca[:, 0],\n",
    "    X_filtered_pca[:, 1],\n",
    "    c=y_train_filtered,\n",
    "    cmap='viridis',\n",
    "    alpha=0.6\n",
    ")\n",
    "axes[1].set_title(\"After Outlier Removal\")\n",
    "axes[1].set_xlabel(\"PCA Component 1\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA().fit(train_features_multiple)\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')\n",
    "plt.title(\"Explained Variance of Combined Features (Three Modalities)\")\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance Threshold')\n",
    "plt.axhline(y=0.99, color='g', linestyle='--', label='99% Variance Threshold')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Random Forest Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "*** Reference: I learnt how to code a basic Random Forest Tree model by following along this video: https://www.youtube.com/watch?v=kFwe2ZZU7yw and therefore the cell below is very similar to the code in the video. All cells and changes before and after are my own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split=min_samples_split\n",
    "        self.max_depth=max_depth\n",
    "        self.n_features=n_features\n",
    "        self.root=None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for thr in thresholds:\n",
    "                gain = self._information_gain(y, X_column, thr)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "        \n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
    "\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log(p) for p in ps if p>0])\n",
    "\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "        \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2, n_feature=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth=max_depth\n",
    "        self.min_samples_split=min_samples_split\n",
    "        self.n_features=n_feature\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            tree = DecisionTree(max_depth=self.max_depth,\n",
    "                            min_samples_split=self.min_samples_split,\n",
    "                            n_features=self.n_features)\n",
    "            X_sample, y_sample = self._bootstrap_samples(X, y)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def _bootstrap_samples(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        return X[idxs], y[idxs]\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        most_common = counter.most_common(1)[0][0]\n",
    "        return most_common\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        tree_preds = np.swapaxes(predictions, 0, 1)\n",
    "        predictions = np.array([self._most_common_label(pred) for pred in tree_preds])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_reduced = pca.fit_transform(train_features_multiple)\n",
    "X_test_reduced = pca.transform(test_features_multiple)\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "clf = RandomForest(n_trees=30)\n",
    "clf.fit(X_train_reduced, train_label_np)\n",
    "predictions = clf.predict(X_test_reduced)\n",
    "\n",
    "acc =  accuracy(test_label_np, predictions)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamter Script for Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_trees': [10, 20, 50],  \n",
    "    'max_depth': [None, 10, 20], \n",
    "    'min_samples_split': [2, 5, 10] \n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Training with params: {params}\")\n",
    "    rf = RandomForest(\n",
    "        n_trees=params['n_trees'],\n",
    "        max_depth=params['max_depth'] if params['max_depth'] is not None else float('inf'),\n",
    "        min_samples_split=params['min_samples_split']\n",
    "    )\n",
    "    rf.fit(X_train_reduced, train_label_np.flatten())\n",
    "    \n",
    "    y_pred = rf.predict(X_test_reduced)\n",
    "    accuracy = accuracy_score(test_label_np.flatten(), y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'n_trees': params['n_trees'],\n",
    "        'max_depth': params['max_depth'],\n",
    "        'min_samples_split': params['min_samples_split'],\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "best_result = results_df.loc[results_df['accuracy'].idxmax()]\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(best_result)\n",
    "print(\"\\nClassification Report for Best Model:\")\n",
    "best_rf = RandomForest(\n",
    "    n_trees=int(best_result['n_trees']),\n",
    "    max_depth=int(best_result['max_depth']) if best_result['max_depth'] != 'None' else float('inf'),\n",
    "    min_samples_split=int(best_result['min_samples_split'])\n",
    ")\n",
    "best_rf.fit(X_train_reduced, train_label_np)\n",
    "y_pred_best = best_rf.predict(X_test_reduced)\n",
    "print(classification_report(test_label_np, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "n_trees_range = [10, 20, 50, 100]\n",
    "max_depth = 10\n",
    "min_samples_split = 2\n",
    "n_features = None \n",
    "\n",
    "accuracy_scores = []\n",
    "\n",
    "for n_trees in n_trees_range:\n",
    "    custom_rf = RandomForest(n_trees=n_trees, max_depth=max_depth,\n",
    "                             min_samples_split=min_samples_split, n_feature=n_features)\n",
    "    custom_rf.fit(X_train_reduced, train_label_np)\n",
    "    \n",
    "    predictions = custom_rf.predict(X_test_reduced)\n",
    "    \n",
    "    accuracy = accuracy_score(test_label_np, predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(n_trees_range, accuracy_scores, marker='o')\n",
    "plt.title('Convergence Speed of Custom Random Forest')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(train_features_multiple, train_label_np)\n",
    "predictions = model.predict(test_features_multiple)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(test_label_np, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_label_np, predictions))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_label_np, predictions))\n",
    "\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_estimators_range = [10, 20, 50, 100, 200, 300]\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for n in n_estimators_range:\n",
    "    model = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    model.fit(train_features_multiple, train_label_np)\n",
    "    predictions = model.predict(test_features_multiple)\n",
    "    \n",
    "    acc = accuracy_score(test_label_np, predictions) \n",
    "    accuracies.append(acc)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_estimators_range, accuracies, marker='o', linestyle='-')\n",
    "plt.title('Convergence Speed of Random Forest')\n",
    "plt.xlabel('Number of Trees (n_estimators)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Custom RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split=min_samples_split\n",
    "        self.max_depth=max_depth\n",
    "        self.n_features=n_features\n",
    "        self.root=None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "        \n",
    "\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_labels = len(y), len(np.unique(y))\n",
    "\n",
    "        if depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split:\n",
    "            return Node(value=self._most_common_label(y))\n",
    "\n",
    "        feat_idxs = np.random.choice(X.shape[1], self.n_features, replace=False)\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        if best_feature is None:\n",
    "            return Node(value=self._most_common_label(y))\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "        \n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return Node(value=self._most_common_label(y))\n",
    "\n",
    "        left = self._grow_tree(X[left_idxs], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs], y[right_idxs], depth + 1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for thr in thresholds:\n",
    "                gain = self._information_gain(y, X_column, thr)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "        \n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
    "\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y) \n",
    "        ps = hist / len(y)\n",
    "        return -np.sum(ps * np.log(ps + 1e-10))\n",
    "\n",
    "\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "        \n",
    "\n",
    "class RandomForestImproved:\n",
    "    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2, n_features=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "        self.trees = []\n",
    "        self.tree_weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        results = Parallel(n_jobs=-1)(\n",
    "            delayed(self._train_tree_with_weight)(X, y) for _ in range(self.n_trees)\n",
    "        )\n",
    "        self.trees, self.tree_weights = zip(*results)\n",
    "\n",
    "    def _train_tree_with_weight(self, X, y):\n",
    "        tree = DecisionTree(\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            n_features=self.n_features\n",
    "        )\n",
    "        X_sample, y_sample, oob_idxs = self._bootstrap_samples(X, y)\n",
    "        tree.fit(X_sample, y_sample)\n",
    "\n",
    "        if len(oob_idxs) > 0:\n",
    "            oob_pred = tree.predict(X[oob_idxs])\n",
    "            oob_acc = np.mean(oob_pred == y[oob_idxs])\n",
    "        else:\n",
    "            oob_acc = 1.0\n",
    "\n",
    "        return tree, oob_acc\n",
    "\n",
    "    def _bootstrap_samples(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        oob_idxs = list(set(range(n_samples)) - set(idxs))\n",
    "        return X[idxs], y[idxs], oob_idxs\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        assert tree_preds.ndim == 2, f\"tree_preds shape is {tree_preds.shape}, expected 2D\"\n",
    "        \n",
    "        assert len(self.tree_weights) == tree_preds.shape[0], (\n",
    "            f\"Tree weights length ({len(self.tree_weights)}) does not match number of trees ({tree_preds.shape[0]})\"\n",
    "        )\n",
    "        \n",
    "        n_samples = tree_preds.shape[1]\n",
    "        vote_tally = np.zeros((n_samples, 20)) \n",
    "        \n",
    "        for tree_idx, tree_weight in enumerate(self.tree_weights):\n",
    "            for sample_idx, pred in enumerate(tree_preds[tree_idx]):\n",
    "                vote_tally[sample_idx, pred - 1] += tree_weight\n",
    "        \n",
    "        predictions = np.argmax(vote_tally, axis=1)\n",
    "        return predictions + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "outlier_mask = iso.fit_predict(train_features_multiple) == 1\n",
    "\n",
    "X_train_filtered = train_features_multiple[outlier_mask]\n",
    "y_train_filtered = train_label_np[outlier_mask]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_filtered)\n",
    "X_test_scaled = scaler.transform(test_features_multiple)\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_reduced = pca.fit_transform(X_train_scaled)\n",
    "X_test_reduced = pca.transform(X_test_scaled)\n",
    "\n",
    "clf = RandomForestImproved(n_trees=150, max_depth=35, min_samples_split=2)\n",
    "clf.fit(X_train_reduced, y_train_filtered)\n",
    "\n",
    "predictions = clf.predict(X_test_reduced)\n",
    "accuracy = accuracy_score(test_label_np, predictions)\n",
    "\n",
    "print(f\"Custom Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### New Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "y_train = train_label_np.flatten()\n",
    "y_test = test_label_np.flatten()\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train = pca.fit_transform(train_features_multiple)\n",
    "X_test = pca.transform(test_features_multiple)\n",
    "\n",
    "param_grid = {\n",
    "    'n_trees': [50, 80, 100],\n",
    "    'max_depth': [20, 35],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Training with params: {params}\")\n",
    "    rf = RandomForestImproved(\n",
    "        n_trees=params['n_trees'],\n",
    "        max_depth=params['max_depth'] if params['max_depth'] is not None else float('inf'),\n",
    "        min_samples_split=params['min_samples_split']\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'n_trees': params['n_trees'],\n",
    "        'max_depth': params['max_depth'],\n",
    "        'min_samples_split': params['min_samples_split'],\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "    \n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "best_result = results_df.loc[results_df['accuracy'].idxmax()]\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(best_result)\n",
    "print(\"\\nClassification Report for Best Model:\")\n",
    "best_rf = RandomForestImproved(\n",
    "    n_trees=int(best_result['n_trees']),\n",
    "    max_depth=int(best_result['max_depth']) if best_result['max_depth'] != 'None' else float('inf'),\n",
    "    min_samples_split=int(best_result['min_samples_split'])\n",
    ")\n",
    "best_rf.fit(X_train, y_train)\n",
    "y_pred_best = best_rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, roc_curve, auc, precision_recall_curve,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "\n",
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "outlier_mask = iso.fit_predict(train_features_multiple) == 1\n",
    "\n",
    "X_train_filtered = train_features_multiple[outlier_mask]\n",
    "y_train_filtered = train_label_np[outlier_mask]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_filtered)\n",
    "X_test_scaled = scaler.transform(test_features_multiple)\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_reduced = pca.fit_transform(X_train_scaled)\n",
    "X_test_reduced = pca.transform(X_test_scaled)\n",
    "\n",
    "clf = RandomForestImproved(n_trees=150, max_depth=35, min_samples_split=2)\n",
    "clf.fit(X_train_reduced, y_train_filtered)\n",
    "\n",
    "\n",
    "y_prob = clf.predict_proba(X_test_reduced) \n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "y_pred = y_pred + 1 \n",
    "y_true = test_label_np\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision (Macro): {precision:.2f}\")\n",
    "print(f\"Recall (Macro): {recall:.2f}\")\n",
    "print(f\"F1 Score (Macro): {f1:.2f}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.gca().invert_yaxis()\n",
    "tick_marks = np.arange(conf_matrix.shape[0]) + 1\n",
    "plt.xticks(tick_marks - 1, tick_marks)\n",
    "plt.yticks(tick_marks - 1, tick_marks)\n",
    "plt.title(\"Confusion Matrix of Custom Improved Model\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stats for SK Learn RF Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, roc_curve, auc, precision_recall_curve,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train_reduced, y_train_filtered)\n",
    "\n",
    "\n",
    "y_prob = clf.predict_proba(X_test_reduced)\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "y_pred = y_pred + 1\n",
    "y_true = test_label_np\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision (Macro): {precision:.2f}\")\n",
    "print(f\"Recall (Macro): {recall:.2f}\")\n",
    "print(f\"F1 Score (Macro): {f1:.2f}\")\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.gca().invert_yaxis()\n",
    "tick_marks = np.arange(conf_matrix.shape[0]) + 1\n",
    "plt.xticks(tick_marks - 1, tick_marks)\n",
    "plt.yticks(tick_marks - 1, tick_marks)\n",
    "plt.title(\"Confusion Matrix of SK Learn Random Forest\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SK Learn Logistic Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, roc_curve, auc, precision_recall_curve,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_reduced, y_train_filtered)\n",
    "\n",
    "\n",
    "y_prob = clf.predict_proba(X_test_reduced)\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "y_pred = y_pred + 1\n",
    "y_true = test_label_np\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision (Macro): {precision:.2f}\")\n",
    "print(f\"Recall (Macro): {recall:.2f}\")\n",
    "print(f\"F1 Score (Macro): {f1:.2f}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.gca().invert_yaxis()\n",
    "tick_marks = np.arange(conf_matrix.shape[0]) + 1\n",
    "plt.xticks(tick_marks - 1, tick_marks)\n",
    "plt.yticks(tick_marks - 1, tick_marks)\n",
    "plt.title(\"Confusion Matrix of SK Learn Logistic Regression\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sk Learn Convergence Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_estimators_range = [10, 20, 50, 100]\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for n in n_estimators_range:\n",
    "    model = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    model.fit(train_features_multiple, train_label_np)\n",
    "    predictions = model.predict(test_features_multiple)\n",
    "    \n",
    "    acc = accuracy_score(test_label_np, predictions)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_estimators_range, accuracies, marker='o', linestyle='-')\n",
    "plt.title('Convergence Speed of Random Forest')\n",
    "plt.xlabel('Number of Trees (n_estimators)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Paradigm: Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Model for active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_labels = len(y), len(np.unique(y))\n",
    "\n",
    "        if depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split:\n",
    "            return Node(value=self._most_common_label(y))\n",
    "\n",
    "        feat_idxs = np.random.choice(X.shape[1], self.n_features, replace=False)\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        if best_feature is None:\n",
    "            return Node(value=self._most_common_label(y))\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs], y[right_idxs], depth + 1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, X_column, threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = threshold\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        return parent_entropy - child_entropy\n",
    "\n",
    "    def _split(self, X_column, threshold):\n",
    "        left_idxs = np.argwhere(X_column <= threshold).flatten()\n",
    "        right_idxs = np.argwhere(X_column > threshold).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum(ps * np.log(ps + 1e-10))\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        return counter.most_common(1)[0][0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class RandomForestActive:\n",
    "    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2, n_features=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = Parallel(n_jobs=-1)(\n",
    "            delayed(self._train_tree)(X, y) for _ in range(self.n_trees)\n",
    "        )\n",
    "\n",
    "    def _train_tree(self, X, y):\n",
    "        tree = DecisionTree(\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            n_features=self.n_features\n",
    "        )\n",
    "        X_sample, y_sample = self._bootstrap_samples(X, y)\n",
    "        tree.fit(X_sample, y_sample)\n",
    "        return tree\n",
    "\n",
    "    def _bootstrap_samples(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        return X[idxs], y[idxs]\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        predictions = np.array([Counter(tree_preds[:, i]).most_common(1)[0][0] for i in range(X.shape[0])])\n",
    "        return predictions\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "\n",
    "        all_classes = set(np.concatenate(tree_preds.T))\n",
    "        n_classes = len(all_classes)\n",
    "\n",
    "        class_to_index = {cls: idx for idx, cls in enumerate(sorted(all_classes))}\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        prob_matrix = np.zeros((n_samples, n_classes))\n",
    "\n",
    "        for sample_idx in range(n_samples):\n",
    "            class_counts = Counter(tree_preds[:, sample_idx])\n",
    "            for cls, count in class_counts.items():\n",
    "                prob_matrix[sample_idx, class_to_index[cls]] = count\n",
    "\n",
    "        prob_matrix = prob_matrix / np.sum(prob_matrix, axis=1, keepdims=True)\n",
    "        return prob_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = train_features_multiple, train_label_np\n",
    "X_test, y_test = test_features_multiple, test_label_np\n",
    "\n",
    "initial_size = 20\n",
    "X_train, X_pool, y_train, y_pool = train_test_split(X, y, train_size=initial_size, stratify=y, random_state=42)\n",
    "\n",
    "model = RandomForestActive(n_trees=10, max_depth=10, min_samples_split=2)\n",
    "\n",
    "n_iterations = 10\n",
    "batch_size = 10\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    pool_probs = model.predict_proba(X_pool)\n",
    "\n",
    "    entropy = -np.sum(pool_probs * np.log(pool_probs + 1e-9), axis=1)\n",
    "\n",
    "    uncertain_indices = np.argsort(entropy)[-batch_size:]\n",
    "\n",
    "    X_new = X_pool[uncertain_indices]\n",
    "    y_new = y_pool[uncertain_indices]\n",
    "    X_train = np.vstack((X_train, X_new))\n",
    "    y_train = np.hstack((y_train, y_new))\n",
    "\n",
    "    mask = np.ones(len(X_pool), dtype=bool)\n",
    "    mask[uncertain_indices] = False\n",
    "    X_pool = X_pool[mask]\n",
    "    y_pool = y_pool[mask]\n",
    "\n",
    "    test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(f\"Iteration {iteration + 1}, Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "final_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "print(f\"Final Test Accuracy: {final_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Active Learning for Old"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
